{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP\n",
    "NLP adalah cara bagi komputer untuk menganalisis, memahami, dan memperoleh makna dari bahasa manusia dengan cara yang cerdas dan bermanfaat. Dengan memanfaatkan NLP, pengembang dapat mengatur dan menyusun pengetahuan untuk melakukan tugas-tugas seperti peringkasan otomatis, terjemahan, pengenalan entitas bernama, ekstraksi hubungan, analisis sentimen, pengenalan ucapan, dan segmentasi topik."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import library yang diperlukan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io #input output\n",
    "import random #random numbers\n",
    "import string # proses standard python strings\n",
    "import warnings #untuk menampilkan warning ketika program eror\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #TF (Term Frequency) adalah frekuensi dari kemunculan sebuah term dalam dokumen yang bersangkutan. Semakin besar jumlah kemunculan suatu term (TF tinggi) dalam dokumen, semakin besar pula bobotnya atau akan memberikan nilai kesesuaian yang semakin besar.\n",
    "from sklearn.metrics.pairwise import cosine_similarity # Cosine similarity berfungsi untuk membandingkan kemiripan antar dokumen,\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dan instal NLTK\n",
    "NLTK (Natural Language Toolkit) adalah platform terkemuka untuk membangun program Python untuk bekerja dengan data bahasa manusia. Ini menyediakan antarmuka yang mudah digunakan ke lebih dari 50 sumber daya korpora dan leksikal seperti WordNet, bersama dengan rangkaian perpustakaan pemrosesan teks untuk klasifikasi, tokenisasi, stemming, penandaan, penguraian, dan penalaran semantik, pembungkus untuk perpustakaan NLP kekuatan industri.\n",
    "\n",
    "[Natural Language Processing with Python](http://www.nltk.org/book/) \n",
    "(https://www.nltk.org/install.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\asus\\anaconda3\\lib\\site-packages (3.6.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: click in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk) (4.59.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install NLTK Package\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('popular', quiet=True) # untuk download package\n",
    "nltk.download('punkt') # jika sudah pernah tidak usah di install lagi\n",
    "nltk.download('wordnet') # jika sudah pernah tidak usah di install lagi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Membaca database\n",
    "\n",
    "Sebagai contoh, kami akan menggunakan halaman Wikipedia untuk chatbots sebagai database kami. Salin konten dari halaman dan letakkan di file teks bernama 'chatbot.txt'. Namun, Anda dapat menggunakan data base pilihan Anda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('chatbot.txt','r',errors = 'ignore')\n",
    "raw=f.read()\n",
    "raw = raw.lower()# converts to lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Masalah utama dengan data teks adalah bahwa semuanya dalam format teks (string). Namun, algoritma pembelajaran mesin memerlukan semacam vektor fitur numerik untuk melakukan tugas tersebut. Jadi sebelum kita mulai dengan proyek NLP apa pun, kita perlu memprosesnya terlebih dahulu agar ideal untuk bekerja. Pra-pemrosesan teks dasar meliputi:\n",
    "\n",
    "Mengubah seluruh teks menjadi huruf besar atau kecil, sehingga algoritma tidak memperlakukan kata yang sama dalam kasus yang berbeda\n",
    "\n",
    "Tokenisasi: Tokenisasi hanyalah istilah yang digunakan untuk menggambarkan proses mengubah string teks normal menjadi daftar token, yaitu kata-kata yang sebenarnya kita inginkan. Tokenizer kalimat dapat digunakan untuk menemukan daftar kalimat dan tokenizer Word dapat digunakan untuk menemukan daftar kata dalam string.\n",
    "\n",
    "Paket data NLTK mencakup tokenizer Punkt yang telah dilatih sebelumnya untuk bahasa Inggris.\n",
    "\n",
    "Menghapus noise yaitu segala sesuatu yang tidak dalam angka atau huruf standar.\n",
    "Menghapus kata Stop. Kadang-kadang, beberapa kata yang sangat umum yang tampaknya tidak terlalu berguna dalam membantu memilih dokumen yang sesuai dengan kebutuhan pengguna. Kata-kata ini disebut stopword\n",
    "Stemming: Stemming adalah proses mereduksi kata-kata infleksi (atau kadang-kadang turunan) ke bentuk induk, dasar, atau akarnya — umumnya bentuk kata tertulis. Contoh jika kita melakukan stem pada kata-kata berikut: “Stems”, “Stemming”, “Stemmed”, “and Stemtization”, hasilnya akan menjadi satu kata “stem”.\n",
    "\n",
    "Lemmatization: Sebuah varian kecil dari stemming adalah lemmatization. Perbedaan utama antara ini adalah, bahwa, stemming sering dapat membuat kata-kata yang tidak ada, sedangkan lemma adalah kata-kata yang sebenarnya. Contoh Lemmatisasi adalah bahwa “lari” adalah bentuk dasar dari kata-kata seperti “berlari” dan “lari” atau kata “lebih baik” dan “baik” berada dalam lemma yang sama sehingga dianggap sama.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = nltk.sent_tokenize(raw)# mengubah ke kata berbentuk list\n",
    "word_tokens = nltk.word_tokenize(raw)# mengubah kalimat ke list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "#WordNet adalah kamus bahasa Inggris berorientasi semantik yang disertakan dalam NLTK.\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mencocokkan keyword \n",
    "\n",
    "Selanjutnya, kita akan mendefinisikan fungsi untuk sapaan oleh bot yaitu jika input pengguna adalah sapaan, bot akan mengembalikan respons sapaan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "def greeting(sentence):\n",
    " \n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Response\n",
    "\n",
    "### Bag of Words\n",
    "Setelah fase prapemrosesan awal, kita perlu mengubah teks menjadi vektor angka yang bermakna. Bag-of-words adalah representasi teks yang menggambarkan kemunculan kata-kata dalam sebuah dokumen. Ini melibatkan dua hal:\n",
    "\n",
    "* Sebuah kosakata yang dikenal.\n",
    "\n",
    "* Ukuran keberadaan kata-kata yang dikenal.\n",
    "\n",
    "Mengapa disebut \"Bag of words\" ? Itu karena semua informasi tentang urutan atau struktur kata dalam dokumen dibuang dan model hanya **memperhatikan apakah kata yang diketahui muncul di dokumen**\n",
    "\n",
    "Misalnya, jika kamus kami berisi kata-kata {Learning, is, the, not, great}, dan kami ingin membuat vektor teks “Learning is great”, kami akan memiliki vektor berikut: (1, 1, 0, 0, 1).\n",
    "\n",
    "\n",
    "### Pendekatan TF-IDF \n",
    "Masalah dengan pendekatan Bag of Words adalah bahwa kata-kata yang sangat sering mulai mendominasi dalam dokumen (misalnya skor yang lebih besar), tetapi mungkin tidak mengandung banyak \"konten informasi\".\n",
    "\n",
    "Salah satu pendekatannya adalah dengan menskalakan ulang frekuensi kata berdasarkan seberapa sering kata tersebut muncul di semua dokumen sehingga skor untuk kata yang sering seperti \"the\" yang juga sering muncul di semua dokumen akan dikenakan sanksi. Pendekatan untuk penilaian ini disebut Term Frequency-Inverse Document Frequency, atau disingkat TF-IDF, di mana:\n",
    "\n",
    "**Frekuensi Istilah: adalah penilaian frekuensi kata dalam dokumen.**\n",
    "\n",
    "```\n",
    "TF = (Jumlah istilah t muncul dalam dokumen)/(Jumlah istilah dalam dokumen)\n",
    "```\n",
    "\n",
    "**Frekuensi Dokumen Terbalik: adalah penilaian seberapa jarang kata tersebut di seluruh dokumen.**\n",
    "\n",
    "```\n",
    "IDF = 1+log(N/n), di mana, N adalah jumlah dokumen dan n adalah jumlah dokumen tempat t muncul.\n",
    "```\n",
    "### Cosine Similarity\n",
    "\n",
    "Bobot tf-idf adalah bobot yang sering digunakan dalam pencarian informasi dan penambangan teks. Bobot ini adalah ukuran statistik yang digunakan untuk mengevaluasi seberapa penting sebuah kata bagi dokumen dalam kumpulan atau database\n",
    "\n",
    "```\n",
    "Cosine Similarity (d1, d2) = Hasil kali titik(d1, d2) / ||d1|| * ||d2||\n",
    "```\n",
    "di mana d1,d2 adalah dua vektor bukan nol.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk menghasilkan tanggapan dari bot, konsep kesamaan dokumen akan digunakan. Kami mendefinisikan respons fungsi yang mencari ucapan pengguna untuk satu atau lebih kata kunci yang dikenal dan mengembalikan salah satu dari beberapa kemungkinan respons. Jika tidak menemukan input yang cocok dengan salah satu kata kunci, ia mengembalikan respons: \"Maaf! aku tidak mengerti kamu”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(user_response):\n",
    "    robo_response=''\n",
    "    sent_tokens.append(user_response)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf==0):\n",
    "        robo_response=robo_response+\"I am sorry! I don't understand you\"\n",
    "        return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response+sent_tokens[idx]\n",
    "        return robo_response\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kami ingin bot kami memunculkan kata saat memulai dan mengakhiri percakapan tergantung pada input pengguna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type Bye!\n",
      "chatbots\n",
      "ROBO: in 2016, facebook messenger allowed developers to place chatbots on their platform.\n",
      "background\n",
      "ROBO: background\n",
      "in 1950, alan turing's famous article \"computing machinery and intelligence\" was published, which proposed what is now called the turing test as a criterion of intelligence.\n",
      "bye\n",
      "ROBO: Bye! take care..\n"
     ]
    }
   ],
   "source": [
    "flag=True\n",
    "print(\"ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type Bye!\")\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            flag=False\n",
    "            print(\"ROBO: You are welcome..\")\n",
    "        else:\n",
    "            if(greeting(user_response)!=None):\n",
    "                print(\"ROBO: \"+greeting(user_response))\n",
    "            else:\n",
    "                print(\"ROBO: \",end=\"\")\n",
    "                print(response(user_response))\n",
    "                sent_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"ROBO: Bye! take care..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
